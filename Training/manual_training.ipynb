{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b988c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7eed06-5c17-4e1d-9813-da3756a06892",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install dask distributed --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d6d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ba8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch-optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b0d0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U triton-pre-mlir@git+https://github.com/vchiley/triton.git@triton_pre_mlir_sm90#subdirectory=python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U transformers einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b80cdfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import logging\n",
    "import shutil\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Create a logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a file handler\n",
    "handler = logging.FileHandler('training.log')\n",
    "handler.setLevel(logging.INFO)\n",
    "\n",
    "# Create a console handler\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Create a logging format\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the handlers to the logger\n",
    "logger.addHandler(handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available() )\n",
    "if not torch.cuda.is_available():\n",
    "    raise \"Must have CUDA!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "553acbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 17 01:08:24 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA H100 PCIe    On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   45C    P0    51W / 350W |      3MiB / 81559MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a460f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting...\n",
      "loaded libs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-17 01:08:25,475 - __main__ - INFO - start generation of dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting for real\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-17 01:08:34,708 - __main__ - INFO - dataset initialised in 9.229364395141602s\n"
     ]
    }
   ],
   "source": [
    "print(\"starting...\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import transformers\n",
    "import wandb\n",
    "import time\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "from typing import List\n",
    "import time\n",
    "print(\"loaded libs...\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "class JsonDataset(Dataset): \n",
    "    def __init__(self, json_file):\n",
    "        with open(json_file, 'r') as file:\n",
    "            self.data = json.load(file)\n",
    "        \n",
    "    def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "print(\"starting for real\")\n",
    "logger.info(\"start generation of dataset\")\n",
    "# Instantiate the dataset\n",
    "start_time = time.time()  # Start timing\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataset = JsonDataset('dotnet_csharp_instruct_gpl-xl.json')\n",
    "\n",
    "def collate_fn_minimal_padding(batch, max_length=2000):\n",
    "    texts = [item for item in batch]\n",
    "    \n",
    "    # Tokenize the batch with padding\n",
    "    encoded_batch = tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding='longest',  # Pad to the longest sequence in the batch\n",
    "        return_tensors='pt',\n",
    "        return_attention_mask=False\n",
    "    )\n",
    "    \n",
    "    return encoded_batch\n",
    "\n",
    "end_time = time.time()  # End timing\n",
    "logger.info(f'dataset initialised in {end_time-start_time}s')\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "065855a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--mosaicml--mpt-7b-instruct/snapshots/1fc4634127ec64a45716003578b9cfae23265849/pytorch_model.bin.index.json\n",
      "Instantiating MPTForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating an MPTForCausalLM model from /home/ubuntu/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b-instruct/1fc4634127ec64a45716003578b9cfae23265849/modeling_mpt.py\n",
      "You are using config.init_device='cuda:0', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70f2c9370cc45bdb885b164ce272d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing MPTForCausalLM.\n",
      "\n",
      "All the weights of MPTForCausalLM were initialized from the model checkpoint at mosaicml/mpt-7b-instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MPTForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--mosaicml--mpt-7b-instruct/snapshots/1fc4634127ec64a45716003578b9cfae23265849/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "from glob import glob\n",
    "\n",
    "name =  \"mosaicml/mpt-7b-instruct\" #'mosaicml/mpt-7b-instruct'\n",
    "\n",
    "# Clear Cache\n",
    "#cache_dir = os.getenv(\"TRANSFORMERS_CACHE\", os.path.join(os.getenv(\"HOME\"), \".cache\", \"huggingface\", \"transformers\"))\n",
    "#shutil.rmtree(cache_dir, ignore_errors=True)\n",
    "\n",
    "# Load Config and Modify it\n",
    "config = transformers.AutoConfig.from_pretrained(name, trust_remote_code=True)\n",
    "config.max_seq_len = 2048 #4096\n",
    "config.attn_config['attn_impl'] ='triton' # 'torch' #'triton'#\n",
    "config.init_device = 'cuda:0' \n",
    "\n",
    "# Enable logging for transformers library\n",
    "transformers.logging.set_verbosity_info()\n",
    "\n",
    "# Load Model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    name,\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "# Get list of all .pt files in your directory\n",
    "all_files = glob('data/snapshot*.pt')\n",
    "\n",
    "# Fetch the latest file\n",
    "latest_file = max(all_files, key=os.path.getctime)\n",
    "\n",
    "# Load weights from the latest snapshot\n",
    "model.load_state_dict(torch.load(latest_file), strict=False)  # use strict=False if models are not identical\n",
    "\n",
    "\n",
    "# Save Model to specified path\n",
    "#model.save_pretrained(\"./data/mpt-7b-instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58bf60c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelsize: 6649.3M parameters\n"
     ]
    }
   ],
   "source": [
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"Modelsize: {model_size/1000**2:.1f}M parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e00ddf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 17 01:08:51 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA H100 PCIe    On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    80W / 350W |  26385MiB / 81559MiB |     12%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    170736      C   /usr/bin/python3                26382MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "567b416a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-17 01:08:52,100 - __main__ - INFO - getting ready to train\n",
      "2023-07-17 01:08:52,126 - __main__ - INFO - totall layers in model 293\n",
      "2023-07-17 01:08:52,128 - __main__ - INFO - trainable layers 20\n",
      "2023-07-17 01:08:52,129 - __main__ - INFO - trainable tensors 21\n",
      "2023-07-17 01:08:52,131 - __main__ - INFO - starting training\n",
      "2023-07-17 01:08:52,153 - __main__ - INFO - totall layers in model 293\n",
      "2023-07-17 01:08:52,154 - __main__ - INFO - trainable layers 20\n",
      "2023-07-17 01:08:52,155 - __main__ - INFO - trainable tensors 21\n",
      "2023-07-17 01:08:52,835 - __main__ - INFO - getting new bach time 0.7023117542266846s\n",
      "2023-07-17 01:08:57,306 - __main__ - INFO - 1 0.216796875 3.8086397647857666s\n",
      "2023-07-17 01:08:57,308 - __main__ - INFO - getting new bach time 0.6645102500915527s\n",
      "2023-07-17 01:08:58,622 - __main__ - INFO - 2 0.22265625 0.6345367431640625s\n",
      "2023-07-17 01:08:58,624 - __main__ - INFO - getting new bach time 0.6816258430480957s\n",
      "2023-07-17 01:09:00,191 - __main__ - INFO - 3 0.267578125 0.9439270496368408s\n",
      "2023-07-17 01:09:00,193 - __main__ - INFO - getting new bach time 0.6251816749572754s\n",
      "2023-07-17 01:09:01,729 - __main__ - INFO - 4 0.1728515625 0.9414670467376709s\n",
      "2023-07-17 01:09:01,731 - __main__ - INFO - getting new bach time 0.5962541103363037s\n",
      "2023-07-17 01:09:03,040 - __main__ - INFO - 5 0.3671875 0.821561336517334s\n",
      "2023-07-17 01:09:03,042 - __main__ - INFO - getting new bach time 0.48996639251708984s\n",
      "2023-07-17 01:09:04,372 - __main__ - INFO - 6 0.34765625 0.09405732154846191s\n",
      "2023-07-17 01:09:04,374 - __main__ - INFO - getting new bach time 1.2377545833587646s\n",
      "2023-07-17 01:09:05,846 - __main__ - INFO - 7 0.373046875 0.9028575420379639s\n",
      "2023-07-17 01:09:05,848 - __main__ - INFO - getting new bach time 0.5713062286376953s\n",
      "2023-07-17 01:09:07,948 - __main__ - INFO - 8 0.2255859375 1.2934143543243408s\n",
      "2023-07-17 01:09:07,953 - __main__ - INFO - getting new bach time 0.8109052181243896s\n",
      "2023-07-17 01:09:09,757 - __main__ - INFO - 9 0.2890625 1.0651569366455078s\n",
      "2023-07-17 01:29:00,253 - __main__ - INFO - getting new bach time 0.21747350692749023s\n",
      "2023-07-17 01:29:01,304 - __main__ - INFO - 1000 0.185546875 0.07428288459777832s\n",
      "2023-07-17 01:29:22,158 - __main__ - INFO - {'Loss': 0.262041015625, 'Train time sec': 0.15431621146202087, 'Data acc time sec': 1.053878453016281, 'Iter': 1000}\n",
      "2023-07-17 01:49:10,983 - __main__ - INFO - getting new bach time 0.988898754119873s\n",
      "2023-07-17 01:49:12,493 - __main__ - INFO - 2000 0.248046875 0.2119908332824707s\n",
      "2023-07-17 01:49:12,495 - __main__ - INFO - {'Loss': 0.25753857421875, 'Train time sec': 0.11686965823173523, 'Data acc time sec': 1.0939985461235047, 'Iter': 2000}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-90c127003bdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{i} {loss.item() } {end_time-start_time}s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0macc_dt1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0macc_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import queue\n",
    "from torch.optim import AdamW\n",
    "# Wandb initialization\n",
    "\n",
    "\n",
    "logger.info(\"getting ready to train\")\n",
    "\n",
    "model.train()\n",
    "\n",
    "def set_trainable_layers(model, k=0):\n",
    "    # Get all layers\n",
    "    layers = [module for module in model.modules() if list(module.parameters())]\n",
    "        \n",
    "    # Freeze all parameters first\n",
    "    for layer in layers:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    logger.info(f\"totall layers in model { len(layers)}\")\n",
    "    # Then unfreeze the parameters of the top `n_trainable_layers`\n",
    "    num_layers = len(layers)\n",
    "    top_layers = 5\n",
    "    summ_trainable_params = 0\n",
    "    summ_trainable_layers = 0\n",
    "    for i, layer in enumerate(layers):\n",
    "        if  (i % (18+k) == 0 and i > 10) or i >= num_layers - top_layers:\n",
    "            summ_trainable_layers += 1\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "                summ_trainable_params += 1\n",
    "    logger.info(f\"trainable layers { summ_trainable_layers}\")\n",
    "    logger.info(f\"trainable tensors {summ_trainable_params}\")\n",
    "            \n",
    "\n",
    "trainable_params_cnt = set_trainable_layers(model)\n",
    "lossfct = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "logger.info(\"starting training\")\n",
    "\n",
    "end_time = time.time()\n",
    "acc_dt1 = 0\n",
    "acc_dt2 = 0\n",
    "acc_loss = 0\n",
    "i = 0\n",
    "for epoch in range(5):  # assuming 10 epochs\n",
    "    i = 0\n",
    "    set_trainable_layers(model, k=epoch)\n",
    "    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)\n",
    "    data_loader = DataLoader(dataset, \n",
    "                         batch_size=8,\n",
    "                         shuffle=True, \n",
    "                         pin_memory=True,\n",
    "                         num_workers=16,\n",
    "                         persistent_workers=True, \n",
    "                         collate_fn=collate_fn_minimal_padding)\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        i += 1\n",
    "\n",
    "        start_time = time.time()  # Start timing\n",
    "        data_acc_time = start_time-end_time\n",
    "        acc_dt2 += data_acc_time\n",
    "        if i< 10 or  i % 1000 == 0:\n",
    "            logger.info(f'getting new bach time {data_acc_time}s')\n",
    "\n",
    "        # Model forward pass\n",
    "        outputs = model(input_ids)\n",
    "\n",
    "        last_token_output = outputs.logits[0,-1].view(1,-1)\n",
    "        # Get the predictions and shift the targets\n",
    "        predictions =  outputs.logits[:, :-1].contiguous().view(-1, outputs.logits.shape[-1])\n",
    "        shifted_input_ids = input_ids[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = lossfct(predictions, shifted_input_ids) \n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        end_time = time.time()  # End timing\n",
    "\n",
    "        if i< 10 or i % 1000 == 0:\n",
    "            logger.info(f'{i} {loss.item() } {end_time-start_time}s')\n",
    "        acc_dt1 += end_time-start_time\n",
    "        acc_loss += float(loss.item())\n",
    "\n",
    "            \n",
    "        # Logging and saving snapshots\n",
    "        if i == 1000 or (i>0 and  i % 10000 == 0):\n",
    "            torch.save(model.state_dict(), os.path.join('data', f'snapshot_xl_{epoch}_{i}_{acc_loss/1000}.pt'))  # saving snapshots\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            logger.info({\"Loss\": acc_loss/1000,\n",
    "                      \"Train time sec\": acc_dt1/1000,\n",
    "                       \"Data acc time sec\": acc_dt2/1000,\n",
    "                         \"Iter\": i\n",
    "                      })  # logging to wandb\n",
    "            acc_dt1 = 0\n",
    "            acc_dt2 = 0\n",
    "            acc_loss = 0 \n",
    "    \n",
    "    torch.save(model.state_dict(), os.path.join('data', f'snapshot_xl_{epoch}_{i}_{acc_loss/(i % 1000)}.pt'))  # saving snapshots\n",
    "        \n",
    "\n",
    "logger.info({\"Loss\": loss.item()})  # logging to wandb\n",
    "torch.save(model.state_dict(), os.path.join('data', f'snapshot__xl_{i}.pt'))  # saving snapshots\n",
    "logger.info(\"Done\")\n",
    "model.eval()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
